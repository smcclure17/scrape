import requests
from bs4 import BeautifulSoup
import os
import time
import sys
import pandas as pd
##selenium requirements
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support.expected_conditions import presence_of_element_located
from selenium.webdriver.common.action_chains import ActionChains

############################################################################################################
## setup
############################################################################################################

os.chdir("C:\\xampp\\htdocs\\airbnb") #set curr directory

## set behavior of driver/browser -- use chrome driver
options = webdriver.ChromeOptions()
options.add_argument('--ignore-certificate-errors')
options.add_argument('--incognito')
options.add_argument('--headless')
options.add_argument("--disable-notifications")
## attempt to set download path and bypass save dialog box
options.add_experimental_option("prefs", {
        "download.default_directory": "C:\\Users\\Sean McClure\\Downloads\\",
        "download.prompt_for_download": False,
        "download.directory_upgrade": True,
        "safebrowsing_for_trusted_sources_enabled": False,
        "safebrowsing.enabled": False
}) 

############################################################################################################
## scraping
############################################################################################################

## set driver and page load default time
driver = webdriver.Chrome(executable_path='C:/Users/Sean McClure/Downloads/chromedriver', options=options) ##path to where chromedriver is located
wait = WebDriverWait(driver, 10) #timeout after 10 seconds

## fetch page data of specified link
driver.get("https://covid.cdc.gov/covid-data-tracker/#cases_casesper100klast7days")
## table is generated by JSâ€”wait until page has been populated by script
wait.until(presence_of_element_located((By.ID, "btnUSTableExport")))

## could not 'get' button to download -- returns NoneType, no found solution
## scrape table directly instead (see below)
# element = driver.find_element_by_id("btnUSTableExport")
# ActionChains(driver).click(element).perform()

## parse data and locate+store relevent table
page_source = driver.page_source
soup = BeautifulSoup(page_source, "lxml")
table = soup.find_all('table', id='us-cases-table')

## write table to pandas df
state_cases = pd.read_html(str(table))[0]
print(state_cases)
## can write to csv, summarize, etc. from here

driver.close() #close headless browser
print("done")





